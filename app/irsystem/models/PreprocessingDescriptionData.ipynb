{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import string\n",
    "from operator import itemgetter\n",
    "from nltk.stem import PorterStemmer\n",
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    clean = re.compile('<.*?>')\n",
    "    cleantext = re.sub(clean, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.    \n",
    "    Params: {text: String}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    return list(filter(str.strip, list(map(lambda x: x, re.findall(r'[a-zA-Z]*', text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    stemmer=PorterStemmer()\n",
    "    stems = [stemmer.stem(w) for w in tokenize(text)]\n",
    "    return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = str(text)\n",
    "    text = cleanhtml(text)\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    for (index,value) in data['Summary'].items():\n",
    "        value = preprocess_text(value)\n",
    "        value = stem(value)\n",
    "        data.loc[index,'Summary'] = value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_feats = 5000\n",
    "doc_by_vocab = np.empty([len(data), n_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiezhao/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "def build_vectorizer(max_features, stop_words, max_df=0.8, min_df=10, norm='l2'):\n",
    "    \"\"\"Returns a TfidfVectorizer object with the above preprocessing properties.\n",
    "    \n",
    "    Params: {max_features: Integer,\n",
    "             max_df: Float,\n",
    "             min_df: Float,\n",
    "             norm: String,\n",
    "             stop_words: String}\n",
    "    Returns: TfidfVectorizer\n",
    "    \"\"\"\n",
    "    \n",
    "    result = TfidfVectorizer(max_features = max_features, stop_words = stop_words, max_df = max_df, min_df = min_df, norm = norm)\n",
    "    return result\n",
    "\n",
    "data = preprocess(data)\n",
    "tfidf_vec = build_vectorizer(n_feats, \"english\")\n",
    "doc_by_vocab = tfidf_vec.fit_transform([value for _,value in data['Summary'].items()]).toarray()\n",
    "index_to_vocab = {i:v for i, v in enumerate(tfidf_vec.get_feature_names())}\n",
    "movie_index_to_name = data['Title'].to_dict()\n",
    "movie_name_to_index = {v: k for k, v in movie_index_to_name.items()}\n",
    "num_movies = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim(mov1, mov2, input_doc_mat, movie_name_to_index):\n",
    "    \"\"\"Returns a float giving the cosine similarity of \n",
    "       the two movie transcripts.\n",
    "    \n",
    "    Params: {mov1: String,\n",
    "             mov2: String,\n",
    "             input_doc_mat: Numpy Array,\n",
    "             movie_name_to_index: Dict}\n",
    "    Returns: Float (Cosine similarity of the two movie transcripts.)\n",
    "    \"\"\"\n",
    "    idx1 = movie_name_to_index[mov1]\n",
    "    idx2 = movie_name_to_index[mov2]\n",
    "    movie1 = input_doc_mat[idx1,]\n",
    "    movie2 = input_doc_mat[idx2,]\n",
    "    dot_product = np.dot(movie1, movie2)\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophiezhao/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "def build_movie_sims_cos(n_mov, movie_index_to_name, input_doc_mat, movie_name_to_index, input_get_sim_method):\n",
    "    \"\"\"Returns a movie_sims matrix of size (num_movies,num_movies) where for (i,j):\n",
    "        [i,j] should be the cosine similarity between the movie with index i and the movie with index j\n",
    "    \n",
    "    Params: {n_mov: Integer,\n",
    "             movie_index_to_name: Dict,\n",
    "             input_doc_mat: Numpy Array,\n",
    "             movie_name_to_index: Dict,\n",
    "             input_get_sim_method: Function}\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "    result = np.zeros((n_mov, n_mov))\n",
    "    for i in range(n_mov):\n",
    "        for j in range(n_mov):\n",
    "            if i == j:\n",
    "                result[i,j] = 0\n",
    "            else:\n",
    "                mov1 = movie_index_to_name[i]\n",
    "                mov2 = movie_index_to_name[j]\n",
    "                result[i,j] = input_get_sim_method(mov1, mov2, input_doc_mat, movie_name_to_index)\n",
    "    \n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Drama_Title  Summary_Similarity  Genre_Similarity  \\\n",
      "667                     Stranger            0.432997               1.0   \n",
      "415          Sweet Savage Family            0.276500               1.0   \n",
      "722          The Queen of Office            0.237680               1.0   \n",
      "824           The Wedding Scheme            0.218926               1.0   \n",
      "242        Bad Thief, Good Thief            0.212300               1.0   \n",
      "312                  Be Positive            0.212232               1.0   \n",
      "428             You Will Love Me            0.210892               1.0   \n",
      "498                    The Lover            0.208423               1.0   \n",
      "333  Cinderella and Four Knights            0.207837               1.0   \n",
      "32              The Best Chicken            0.205444               1.0   \n",
      "\n",
      "     Network_Similarity     Total  \n",
      "667                   0  1.432997  \n",
      "415                   0  1.276500  \n",
      "722                   0  1.237680  \n",
      "824                   0  1.218926  \n",
      "242                   0  1.212300  \n",
      "312                   0  1.212232  \n",
      "428                   0  1.210892  \n",
      "498                   0  1.208423  \n",
      "333                   0  1.207837  \n",
      "32                    0  1.205444  \n"
     ]
    }
   ],
   "source": [
    "movie_sims_cos = build_movie_sims_cos(num_movies, movie_index_to_name, doc_by_vocab, movie_name_to_index, get_sim)\n",
    "print(movie_sims_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Drama_Title</th>\n",
       "      <th>Summary_Similarity</th>\n",
       "      <th>Genre_Similarity</th>\n",
       "      <th>Network_Similarity</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>General Hospital 2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>Baekmansongi Jangmi / One Million Roses</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>Immortal Classic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>Before and After: Plastic Surgery Clinic</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>Gwaenchanha, Appa Ttal / Itâ€™s Okay, Daddyâ€™...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>The Invisible Man</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>Dr. Gang</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>Yo-jo-suk-nyeo / My Fair Lady</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>My Rosy Life</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>The 3rd Hospital</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Drama_Title  Summary_Similarity  \\\n",
       "1111                                 General Hospital 2                   0   \n",
       "1431            Baekmansongi Jangmi / One Million Roses                   0   \n",
       "833                                    Immortal Classic                   0   \n",
       "1171           Before and After: Plastic Surgery Clinic                   0   \n",
       "966   Gwaenchanha, Appa Ttal / Itâ€™s Okay, Daddyâ€™...                   0   \n",
       "1288                                  The Invisible Man                   0   \n",
       "1301                                           Dr. Gang                   0   \n",
       "1430                      Yo-jo-suk-nyeo / My Fair Lady                   0   \n",
       "1328                                       My Rosy Life                   0   \n",
       "774                                    The 3rd Hospital                   0   \n",
       "\n",
       "      Genre_Similarity  Network_Similarity  Total  \n",
       "1111               1.0                   0    1.0  \n",
       "1431               0.5                   0    0.5  \n",
       "833                0.5                   0    0.5  \n",
       "1171               0.5                   0    0.5  \n",
       "966                0.5                   0    0.5  \n",
       "1288               0.5                   0    0.5  \n",
       "1301               0.5                   0    0.5  \n",
       "1430               0.5                   0    0.5  \n",
       "1328               0.5                   0    0.5  \n",
       "774                0.5                   0    0.5  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_match(n_mov, movie_sims_cos, data, movie_index_to_name, movie_name_to_index, dramas_enjoyed, dramas_disliked, preferred_genres, preferred_network, num_results):\n",
    "    feature_list = ['Summary_Similarity', 'Genre_Similarity', 'Network_Similarity', 'Total']\n",
    "    result = pd.DataFrame(0, index=np.arange(n_mov), columns=feature_list)\n",
    "    genres = set()\n",
    "    preferred_genres = [preprocess_text(value) for value in preferred_genres]\n",
    "    genres.update(preferred_genres)\n",
    "    for drama in dramas_enjoyed:\n",
    "        if drama in movie_name_to_index.keys():\n",
    "            index = movie_name_to_index[drama]\n",
    "            sim = movie_sims_cos[index,:]\n",
    "            result['Summary_Similarity']+= pd.Series(sim)\n",
    "            \n",
    "    for drama in dramas_disliked:\n",
    "        if drama in movie_name_to_index.keys():\n",
    "            index = movie_name_to_index[drama]\n",
    "            sim = movie_sims_cos[index,:]\n",
    "            result['Summary_Similarity']-= pd.Series(sim)\n",
    "            \n",
    "    for index, value in data.iterrows():\n",
    "        gen = str(value['Genre'])\n",
    "        gen = preprocess_text(gen)\n",
    "        drama_genres = set()\n",
    "        drama_genres.update(gen.split())\n",
    "        result.loc[index,'Genre_Similarity'] = len(genres.intersection(drama_genres))/len(genres.union(drama_genres))\n",
    "        if preferred_network == data.iloc[index]['Network']:\n",
    "            result['Network_Similarity']+=1\n",
    "    result['Total'] = result.sum(axis = 1)\n",
    "    result = result.sort_values(by='Total', ascending=False)\n",
    "    result = result[:num_results]\n",
    "    indices =  result.index.tolist()\n",
    "    best_dramas = pd.Series([movie_index_to_name[index] for index in indices],index = result.index)\n",
    "    result.insert(loc=0, column='Drama_Title', value=best_dramas)\n",
    "    result.reset_index()\n",
    "    return result\n",
    "\n",
    "\n",
    "best_match(num_movies, movie_sims_cos, data, movie_index_to_name, movie_name_to_index, ['Legend of the Blue Sea'], [], [\"drama\", 'Medical'], [], 10)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display (n_mov, movie_sims_cos, data, movie_index_to_name, movie_name_to_index, dramas_enjoyed, dramas_disliked, preferred_genres, preferred_network, num_results):\n",
    "    dramas_enjoyed = dramas_enjoyed.split(', ')\n",
    "    print(dramas_enjoyed)\n",
    "    dramas_disliked = dramas_disliked.split(', ')\n",
    "    preferred_genres = preferred_genres.split(', ')\n",
    "    print(preferred_genres)\n",
    "    preferred_network = preferred_network.split(', ')\n",
    "    print(preferred_network)\n",
    "    best = best_match(n_mov, movie_sims_cos, data, movie_index_to_name, movie_name_to_index, dramas_enjoyed, dramas_disliked, preferred_genres, preferred_network, num_results)\n",
    "    title = list(zip(best['Drama_Title'], best[\"Total\"]))\n",
    "    final = [\"Drama Titles: {}\".format(final_title[0]) + \"            \" +\"Total Similarity {}\".format(final_title[1]) for final_title in title]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Doctors', 'Doctor Stranger']\n",
      "['comedy', 'medical']\n",
      "['']\n",
      "['Drama Titles: Golden Time            Total Similarity 0.966399020676921', 'Drama Titles: Emergency Man and Woman            Total Similarity 0.9166374308189303', 'Drama Titles: Heart to Heart            Total Similarity 0.8865865176251356', 'Drama Titles: Deserving of the Name            Total Similarity 0.8437880548076003', 'Drama Titles: Life            Total Similarity 0.7888586814363907', 'Drama Titles: A Poem a Day            Total Similarity 0.7847812771878924', 'Drama Titles: Stranger            Total Similarity 0.7663299825396421', 'Drama Titles: Cute or Crazy            Total Similarity 0.7555331943556618', 'Drama Titles: Kill Me, Heal Me            Total Similarity 0.7437056384942428', 'Drama Titles: Before and After: Plastic Surgery Clinic            Total Similarity 0.7425579797581201']\n",
      "                                   Drama_Title  Summary_Similarity  \\\n",
      "791                                Golden Time            0.299732   \n",
      "634                    Emergency Man and Woman            0.249971   \n",
      "522                             Heart to Heart            0.219920   \n",
      "215                      Deserving of the Name            0.343788   \n",
      "98                                        Life            0.288859   \n",
      "129                               A Poem a Day            0.284781   \n",
      "667                                   Stranger            0.432997   \n",
      "1324                             Cute or Crazy            0.255533   \n",
      "524                           Kill Me, Heal Me            0.243706   \n",
      "1171  Before and After: Plastic Surgery Clinic            0.242558   \n",
      "\n",
      "      Genre_Similarity  Network_Similarity     Total  \n",
      "791           0.666667                   0  0.966399  \n",
      "634           0.666667                   0  0.916637  \n",
      "522           0.666667                   0  0.886587  \n",
      "215           0.500000                   0  0.843788  \n",
      "98            0.500000                   0  0.788859  \n",
      "129           0.500000                   0  0.784781  \n",
      "667           0.333333                   0  0.766330  \n",
      "1324          0.500000                   0  0.755533  \n",
      "524           0.500000                   0  0.743706  \n",
      "1171          0.500000                   0  0.742558  \n"
     ]
    }
   ],
   "source": [
    "print(display(num_movies, movie_sims_cos, data, movie_index_to_name, movie_name_to_index, \"Doctors, Doctor Stranger\", 'black', \"comedy, medical\", '', 10))\n",
    "print(best_match(num_movies, movie_sims_cos, data, movie_index_to_name, movie_name_to_index, [\"Doctors\", \"Doctor Stranger\"], [\"black\"], [\"comedy\", 'Medical'], [], 10))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'app\\\\irsystem\\\\models\\\\Data-Set-Final.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-199-22bec47fba30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Adeyemi Oyemade\\Documents\\info 4300\\sp822_sbz24_ky239_cne27_aao58_CS4300_Flask_template\\app\\irsystem\\models\\search.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'app\\irsystem\\models\\Data-Set-Final.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcleanhtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_html\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'app\\\\irsystem\\\\models\\\\Data-Set-Final.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
